{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnanik/Shared_Task_SemEval2023/blob/main/Prompting_T5/prompt_based_classifier_EDOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS-DcKcYGjHe"
      },
      "outputs": [],
      "source": [
        "  !pip install transformers datasets openprompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VASmD2osgGMT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import scipy as sp\n",
        "import transformers\n",
        "import datasets\n",
        "from transformers import AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EDOS_train_data_file_path = '/content/train_EDOS_80.csv'\n",
        "EDOS_validation_data_file_path = '/content/validation_EDOS_20.csv'\n",
        "dev_data_file_path = '/content/dev_task_a_entries.csv'\n",
        "test_data_file_path = '/content/test_task_a_entries.csv'"
      ],
      "metadata": {
        "id": "M92gzkN95d52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TunCAy6H7IuP"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(EDOS_train_data_file_path)\n",
        "df_validation = pd.read_csv(EDOS_validation_data_file_path)\n",
        "\n",
        "df_dev = pd.read_csv(dev_data_file_path)\n",
        "# df_dev['text'] = df_dev['text'].astype(str).str.lower()\n",
        "df_dev['labels'] = 0 #we do not use these labels. we just need the labels column\n",
        "\n",
        "df_test_codalab = pd.read_csv(test_data_file_path)\n",
        "# df_test_codalab[\"text\"] = df_test_codalab[\"text\"].astype(str).str.lower()\n",
        "df_test_codalab['labels'] = 0 #we do not use these labels. we just need the labels column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iISPHPRbGYZb"
      },
      "source": [
        "### Encoding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jADyWFVzWMLp"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df_train['labels'] = le.fit_transform(df_train['labels'])\n",
        "df_validation['labels'] = le.fit_transform(df_validation['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWikgZ6Xils3",
        "outputId": "885b0736-dc59-44fe-a5e4-1e0dd5a0a9fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Unnamed: 0', 'text', 'labels'],\n",
              "        num_rows: 11200\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['Unnamed: 0', 'text', 'labels'],\n",
              "        num_rows: 2800\n",
              "    })\n",
              "    dev: Dataset({\n",
              "        features: ['rewire_id', 'text', 'labels'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['rewire_id', 'text', 'labels'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "raw_dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train),\n",
        "    \"validation\": Dataset.from_pandas(df_validation),\n",
        "    \"dev\": Dataset.from_pandas(df_dev),\n",
        "    \"test\": Dataset.from_pandas(df_test_codalab)\n",
        "})\n",
        "    \n",
        "\n",
        "raw_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1iGL5weizy4",
        "outputId": "9028b046-f7e8-443b-87b8-fdfac16f34c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"guid\": null,\n",
            "  \"label\": 0,\n",
            "  \"meta\": {},\n",
            "  \"text_a\": \"By pressing show less you have automatically be been registered for incel 'facts' directly to your inbox daily! We apologise for this feature.\",\n",
            "  \"text_b\": \"\",\n",
            "  \"tgt_text\": null\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from openprompt.data_utils import InputExample\n",
        "\n",
        "dataset = {}\n",
        "for split in ['train', 'validation', 'dev', 'test']:\n",
        "    dataset[split] = []\n",
        "    for data in raw_dataset[split]:\n",
        "        input_example = InputExample(text_a = data['text'], label=int(data['labels'])) #, guid=data['__index_level_0__']\n",
        "        dataset[split].append(input_example)\n",
        "print(dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88NLIerMl03m"
      },
      "source": [
        "### Load the PLM related things provided by openprompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CrktWqHn0Hy"
      },
      "outputs": [],
      "source": [
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox2SNck6mC55"
      },
      "source": [
        "### Constructing Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxmPJlA5mCr7"
      },
      "outputs": [],
      "source": [
        "from openprompt.prompts import ManualTemplate\n",
        "template_text = '{\"placeholder\":\"text_a\"}. This text is {\"mask\"}.'\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydb3tn4ctPe-"
      },
      "source": [
        "To better understand how does the template wrap the example, we visualize one instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj2dKXiamCpU",
        "outputId": "b69403e7-a068-4583-e303-14c953c2b11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'text': \"By pressing show less you have automatically be been registered for incel 'facts' directly to your inbox daily! We apologise for this feature.\", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '. This text is', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}], {'label': 0}]\n"
          ]
        }
      ],
      "source": [
        "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
        "print(wrapped_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OJFws3lmCnG"
      },
      "outputs": [],
      "source": [
        "# Now, the wrapped example is ready to be pass into the tokenizer, hence producing the input for language models.\n",
        "# You can use the tokenizer to tokenize the input by yourself, but we recommend using our wrapped tokenizer, which is a wrapped tokenizer tailed for InputExample.\n",
        "# The wrapper has been given if you use our `load_plm` function, otherwise, you should choose the suitable wrapper based on\n",
        "# the configuration in `openprompt.plms.__init__.py`.\n",
        "# Note that when t5 is used for classification, we only need to pass <pad> <extra_id_0> <eos> to decoder.\n",
        "# The loss is calcaluted at <extra_id_0>. Thus passing decoder_max_length=3 saves the space\n",
        "\n",
        "wrapped_t5tokenizer = WrapperClass(max_seq_length=512, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwXUDYFSmCkW",
        "outputId": "8ac59288-f902-41e4-a445-964cfe8fdc85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [938, 13840, 504, 705, 25, 43, 3269, 36, 118, 3366, 21, 16, 7125, 3, 31, 8717, 7, 31, 1461, 12, 39, 16, 2689, 1444, 55, 101, 3, 9, 102, 11697, 7, 15, 21, 48, 1451, 5, 3, 5, 100, 1499, 19, 32099, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'decoder_input_ids': [0, 32099, 0], 'loss_ids': [0, 1, 0]}\n",
            "['▁By', '▁pressing', '▁show', '▁less', '▁you', '▁have', '▁automatically', '▁be', '▁been', '▁registered', '▁for', '▁in', 'cel', '▁', \"'\", 'fact', 's', \"'\", '▁directly', '▁to', '▁your', '▁in', 'box', '▁daily', '!', '▁We', '▁', 'a', 'p', 'ologi', 's', 'e', '▁for', '▁this', '▁feature', '.', '▁', '.', '▁This', '▁text', '▁is', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<pad>', '<extra_id_0>', '<pad>']\n"
          ]
        }
      ],
      "source": [
        "# You can see what a tokenized example looks like by\n",
        "tokenized_example = wrapped_t5tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)\n",
        "print(tokenized_example)\n",
        "print(tokenizer.convert_ids_to_tokens(tokenized_example['input_ids']))\n",
        "print(tokenizer.convert_ids_to_tokens(tokenized_example['decoder_input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cudk84QOt0I0"
      },
      "source": [
        "Let's convert the whole dataset into the input format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CI_6pPDkmCh4"
      },
      "outputs": [],
      "source": [
        "model_inputs = {}\n",
        "for split in ['train', 'validation', 'dev', 'test']:\n",
        "    model_inputs[split] = []\n",
        "    for sample in dataset[split]:\n",
        "        tokenized_example = wrapped_t5tokenizer.tokenize_one_example(mytemplate.wrap_one_example(sample), teacher_forcing=False)\n",
        "        model_inputs[split].append(tokenized_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DAXXInlEw97"
      },
      "source": [
        "### Define a DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w8uKBBKmCfQ",
        "outputId": "ab8c9c18-57fd-432c-9081-54e76a2fdf28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 11200it [00:11, 964.36it/s]\n"
          ]
        }
      ],
      "source": [
        "from openprompt import PromptDataLoader\n",
        "\n",
        "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2c4HjVXuv9M",
        "outputId": "1e93b04f-38b7-4877-f16b-3620da743469"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['not sexist', 'sexist'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "le.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ta7CxcCFO7G"
      },
      "source": [
        "## Define the verbalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLh4X2cCmCcd",
        "outputId": "70500cd0-f2b0-46be-e945-d7deaa515f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[   59,     3,     7, 12135]],\n",
            "\n",
            "        [[    3,     7, 12135,     0]]])\n",
            "tensor([[-0.9641, -0.4802],\n",
            "        [-2.8828, -0.0576]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# In classification, we need to define the verbalizer, which is a mapping from logits on the vocabulary to the final label probability. Let's have a look at the verbalizer details:\n",
        "\n",
        "from openprompt.prompts import ManualVerbalizer\n",
        "import torch\n",
        "\n",
        "# for example the verbalizer contains multiple label words in each class\n",
        "myverbalizer = ManualVerbalizer(tokenizer, num_classes=2,\n",
        "                        label_words=[[\"not sexist\"], [\"sexist\"]])\n",
        "\n",
        "print(myverbalizer.label_words_ids)\n",
        "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and\n",
        "print(myverbalizer.process_logits(logits)) # see what the verbalizer do\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLWuqE0JvZ3E"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnQnH6CNmCaI"
      },
      "outputs": [],
      "source": [
        "from openprompt import PromptForClassification\n",
        "\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
        "if use_cuda:\n",
        "    prompt_model=  prompt_model.cuda()\n",
        "\n",
        "# Now the training is standard\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
        "\n",
        "for epoch in range(4):\n",
        "    tot_loss = 0\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        loss = loss_func(logits, labels)\n",
        "        loss.backward()\n",
        "        tot_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if step %100 ==1:\n",
        "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bjYh1OymCXh",
        "outputId": "007915e0-6bb8-4944-e563-eac007afbe25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 2800it [00:02, 946.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8689285714285714\n"
          ]
        }
      ],
      "source": [
        "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "for step, inputs in enumerate(validation_dataloader):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qxW0pF849-o",
        "outputId": "ae6ed69f-e542-493a-b8b7-4c656a322d17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2800"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(allpreds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5PedA46mCU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd218b6e-7374-4505-dd99-8562472bbe7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8689285714285714\n",
            "F1: 0.8142658503473659\n",
            "Precision: 0.8294948013731851\n",
            "Recall: 0.8020671476137625\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"Accuracy:\", metrics.accuracy_score(alllabels, allpreds))\n",
        "print(\"F1:\",metrics.f1_score(alllabels, allpreds, average=\"macro\"))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(alllabels, allpreds, average=\"macro\"))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(alllabels, allpreds, average=\"macro\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4IIcn3hkaUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6ae8c1-7bd3-4ebc-8e48-e854419be845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 2000it [00:02, 801.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7865\n"
          ]
        }
      ],
      "source": [
        "dev_dataloader = PromptDataLoader(dataset=dataset[\"dev\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "\n",
        "dev_allpreds = []\n",
        "dev_alllabels = []\n",
        "for step, inputs in enumerate(dev_dataloader):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    dev_alllabels.extend(labels.cpu().tolist())\n",
        "    dev_allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "acc = sum([int(i==j) for i,j in zip(dev_allpreds, dev_alllabels)])/len(dev_allpreds)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dev.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iTgLv80t69P",
        "outputId": "3a208142-688c-4c6a-9b33-1ecc68aec32e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0cHTZ-hkZ9Z"
      },
      "outputs": [],
      "source": [
        "df_dev['label_pred'] = le.inverse_transform(dev_allpreds)\n",
        "df_dev.drop(columns=['labels', 'text'],axis=1, inplace=True)\n",
        "df_dev.to_csv('/content/dev_task_a_t5_base.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ1beDxX5Xf-",
        "outputId": "59a469ae-7fca-4a09-ebe5-0b78727dfc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 4000it [00:04, 909.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.77325\n"
          ]
        }
      ],
      "source": [
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "\n",
        "test_allpreds = []\n",
        "test_alllabels = []\n",
        "for step, inputs in enumerate(test_dataloader):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    test_alllabels.extend(labels.cpu().tolist())\n",
        "    test_allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "acc = sum([int(i==j) for i,j in zip(test_allpreds, test_alllabels)])/len(test_allpreds)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT4xrcSteYAL"
      },
      "outputs": [],
      "source": [
        "df_test_codalab['label_pred'] = le.inverse_transform(test_allpreds)\n",
        "df_test_codalab.drop(columns=['labels', 'text'],axis=1, inplace=True)\n",
        "df_test_codalab.to_csv('/content/test_task_a_labeled_t5_large.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}