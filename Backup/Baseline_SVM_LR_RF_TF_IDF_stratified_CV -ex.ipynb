{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnanik/Shared_Task_SemEval2023/blob/main/Baseline_SVM_LR_RF_TF_IDF_stratified_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y95eFkWcxOsb"
      },
      "source": [
        "To Run the code below, the only requirement is to upload the training data file \"train_all_tasks.csv\" to colab (/content/train_all_tasks.csv). The file can be downloaded from CodaLab website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "b529t-5FBI1e"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'cross_validate' from 'sklearn' (c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16528\\1301498364.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'cross_validate' from 'sklearn' (c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\__init__.py)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm, cross_validate\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score,accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "\n",
        "import statistics\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTD7srgTLLVd",
        "outputId": "2dd9fb5e-37f7-4477-df4c-0ea64be66de0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\DearUser\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\DearUser\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\DearUser\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\DearUser\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\DearUser\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faTXJ56hBrQb"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uLokOBEQBRG5",
        "outputId": "66910562-9b75-475b-f102-4b4402ae7a31"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Damn, this writing was pretty chaotic</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yeah, and apparently a bunch of misogynistic v...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text       label\n",
              "0              Damn, this writing was pretty chaotic  not sexist\n",
              "1  Yeah, and apparently a bunch of misogynistic v...  not sexist"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('./data/train_all_tasks.csv')\n",
        "#data = pd.read_csv('/content/train_all_tasks.csv')\n",
        "df = data.filter(['text', 'label_sexist'])\n",
        "df.columns = ['text', 'label']\n",
        "df[\"text\"] = df[\"text\"].astype(str)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-OtdX0_Bvci"
      },
      "source": [
        "### Data pre-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import string\n",
        "#string.punctuation\n",
        "#   '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2yrCsFBAudy"
      },
      "source": [
        "### Emotics Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "L1d815gPy-dZ"
      },
      "outputs": [],
      "source": [
        "# Thanks : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py for Emoticons and\n",
        "# Thanks : https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt)) for list of chat words\n",
        "\n",
        "EMOTICONS = {\n",
        "    u\":‑\\)\":\"Happy face or smiley\",\n",
        "    u\":\\)\":\"Happy face or smiley\",\n",
        "    u\":-\\]\":\"Happy face or smiley\",\n",
        "    u\":\\]\":\"Happy face or smiley\",\n",
        "    u\":-3\":\"Happy face smiley\",\n",
        "    u\":3\":\"Happy face smiley\",\n",
        "    u\":->\":\"Happy face smiley\",\n",
        "    u\":>\":\"Happy face smiley\",\n",
        "    u\"8-\\)\":\"Happy face smiley\",\n",
        "    u\":o\\)\":\"Happy face smiley\",\n",
        "    u\":-\\}\":\"Happy face smiley\",\n",
        "    u\":\\}\":\"Happy face smiley\",\n",
        "    u\":-\\)\":\"Happy face smiley\",\n",
        "    u\":c\\)\":\"Happy face smiley\",\n",
        "    u\":\\^\\)\":\"Happy face smiley\",\n",
        "    u\"=\\]\":\"Happy face smiley\",\n",
        "    u\"=\\)\":\"Happy face smiley\",\n",
        "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":-\\)\\)\":\"Very happy\",\n",
        "    u\":‑\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑c\":\"Frown, sad, andry or pouting\",\n",
        "    u\":c\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
        "    u\":@\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":'‑\\(\":\"Crying\",\n",
        "    u\":'\\(\":\"Crying\",\n",
        "    u\":'‑\\)\":\"Tears of happiness\",\n",
        "    u\":'\\)\":\"Tears of happiness\",\n",
        "    u\"D‑':\":\"Horror\",\n",
        "    u\"D:<\":\"Disgust\",\n",
        "    u\"D:\":\"Sadness\",\n",
        "    u\"D8\":\"Great dismay\",\n",
        "    u\"D;\":\"Great dismay\",\n",
        "    u\"D=\":\"Great dismay\",\n",
        "    u\"DX\":\"Great dismay\",\n",
        "    u\":‑O\":\"Surprise\",\n",
        "    u\":O\":\"Surprise\",\n",
        "    u\":‑o\":\"Surprise\",\n",
        "    u\":o\":\"Surprise\",\n",
        "    u\":-0\":\"Shock\",\n",
        "    u\"8‑0\":\"Yawn\",\n",
        "    u\">:O\":\"Yawn\",\n",
        "    u\":-\\*\":\"Kiss\",\n",
        "    u\":\\*\":\"Kiss\",\n",
        "    u\":X\":\"Kiss\",\n",
        "    u\";‑\\)\":\"Wink or smirk\",\n",
        "    u\";\\)\":\"Wink or smirk\",\n",
        "    u\"\\*-\\)\":\"Wink or smirk\",\n",
        "    u\"\\*\\)\":\"Wink or smirk\",\n",
        "    u\";‑\\]\":\"Wink or smirk\",\n",
        "    u\";\\]\":\"Wink or smirk\",\n",
        "    u\";\\^\\)\":\"Wink or smirk\",\n",
        "    u\":‑,\":\"Wink or smirk\",\n",
        "    u\";D\":\"Wink or smirk\",\n",
        "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":‑\\|\":\"Straight face\",\n",
        "    u\":\\|\":\"Straight face\",\n",
        "    u\":$\":\"Embarrassed or blushing\",\n",
        "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\"O:‑\\)\":\"Angel, saint or innocent\",\n",
        "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
        "    u\"0:3\":\"Angel, saint or innocent\",\n",
        "    u\"0:‑\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
        "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
        "    u\">:‑\\)\":\"Evil or devilish\",\n",
        "    u\">:\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:‑\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:\\)\":\"Evil or devilish\",\n",
        "    u\"3:‑\\)\":\"Evil or devilish\",\n",
        "    u\"3:\\)\":\"Evil or devilish\",\n",
        "    u\">;\\)\":\"Evil or devilish\",\n",
        "    u\"\\|;‑\\)\":\"Cool\",\n",
        "    u\"\\|‑O\":\"Bored\",\n",
        "    u\":‑J\":\"Tongue-in-cheek\",\n",
        "    u\"#‑\\)\":\"Party all night\",\n",
        "    u\"%‑\\)\":\"Drunk or confused\",\n",
        "    u\"%\\)\":\"Drunk or confused\",\n",
        "    u\":-###..\":\"Being sick\",\n",
        "    u\":###..\":\"Being sick\",\n",
        "    u\"<:‑\\|\":\"Dump\",\n",
        "    u\"\\(>_<\\)\":\"Troubled\",\n",
        "    u\"\\(>_<\\)>\":\"Troubled\",\n",
        "    u\"\\(';'\\)\":\"Baby\",\n",
        "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
        "    u\"\\(\\^_-\\)\":\"Wink\",\n",
        "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
        "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
        "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
        "    u\"\\^_\\^\":\"Joyful\",\n",
        "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
        "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
        "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
        "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
        "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;_;\":\"Sad of Crying\",\n",
        "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
        "    u\";_;\":\"Sad or Crying\",\n",
        "    u\";-;\":\"Sad or Crying\",\n",
        "    u\";n;\":\"Sad or Crying\",\n",
        "    u\";;\":\"Sad or Crying\",\n",
        "    u\"Q\\.Q\":\"Sad or Crying\",\n",
        "    u\"T\\.T\":\"Sad or Crying\",\n",
        "    u\"QQ\":\"Sad or Crying\",\n",
        "    u\"Q_Q\":\"Sad or Crying\",\n",
        "    u\"\\(-\\.-\\)\":\"Shame\",\n",
        "    u\"\\(-_-\\)\":\"Shame\",\n",
        "    u\"\\(一一\\)\":\"Shame\",\n",
        "    u\"\\(；一_一\\)\":\"Shame\",\n",
        "    u\"\\(=_=\\)\":\"Tired\",\n",
        "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
        "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
        "    u\"=_\\^=\t\":\"cat\",\n",
        "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
        "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
        "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
        "    u\"\\(\\・\\・?\":\"Confusion\",\n",
        "    u\"\\(?_?\\)\":\"Confusion\",\n",
        "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
        "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
        "    u\"\\^/\\^\":\"Normal Laugh\",\n",
        "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
        "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
        "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
        "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
        "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
        "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
        "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
        "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
        "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
        "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
        "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
        "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
        "    u'\\(-\"-\\)':\"Worried\",\n",
        "    u\"\\(ーー;\\)\":\"Worried\",\n",
        "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
        "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
        "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
        "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
        "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
        "    u\":O o_O\":\"Surprised\",\n",
        "    u\"o_0\":\"Surprised\",\n",
        "    u\"o\\.O\":\"Surpised\",\n",
        "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
        "    u\"oO\":\"Surprised\",\n",
        "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
        "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n",
        "}\n",
        "\n",
        "chat_words_str = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "AFK=Away From Keyboard\n",
        "ASAP=As Soon As Possible\n",
        "ATK=At The Keyboard\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\n",
        "BAK=Back At Keyboard\n",
        "BBL=Be Back Later\n",
        "BBS=Be Back Soon\n",
        "BFN=Bye For Now\n",
        "B4N=Bye For Now\n",
        "BRB=Be Right Back\n",
        "BRT=Be Right There\n",
        "BTW=By The Way\n",
        "B4=Before\n",
        "B4N=Bye For Now\n",
        "CU=See You\n",
        "CUL8R=See You Later\n",
        "CYA=See You\n",
        "FAQ=Frequently Asked Questions\n",
        "FC=Fingers Crossed\n",
        "FWIW=For What It's Worth\n",
        "FYI=For Your Information\n",
        "GAL=Get A Life\n",
        "GG=Good Game\n",
        "GN=Good Night\n",
        "GMTA=Great Minds Think Alike\n",
        "GR8=Great!\n",
        "G9=Genius\n",
        "IC=I See\n",
        "ICQ=I Seek you (also a chat program)\n",
        "ILU=ILU: I Love You\n",
        "IMHO=In My Honest/Humble Opinion\n",
        "IMO=In My Opinion\n",
        "IOW=In Other Words\n",
        "IRL=In Real Life\n",
        "KISS=Keep It Simple, Stupid\n",
        "LDR=Long Distance Relationship\n",
        "LMAO=Laugh My A.. Off\n",
        "LOL=Laughing Out Loud\n",
        "LTNS=Long Time No See\n",
        "L8R=Later\n",
        "MTE=My Thoughts Exactly\n",
        "M8=Mate\n",
        "NRN=No Reply Necessary\n",
        "OIC=Oh I See\n",
        "PITA=Pain In The A..\n",
        "PRT=Party\n",
        "PRW=Parents Are Watching\n",
        "ROFL=Rolling On The Floor Laughing\n",
        "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
        "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
        "SK8=Skate\n",
        "STATS=Your sex and age\n",
        "ASL=Age, Sex, Location\n",
        "THX=Thank You\n",
        "TTFN=Ta-Ta For Now!\n",
        "TTYL=Talk To You Later\n",
        "U=You\n",
        "U2=You Too\n",
        "U4E=Yours For Ever\n",
        "WB=Welcome Back\n",
        "WTF=What The F...\n",
        "WTG=Way To Go!\n",
        "WUF=Where Are You From?\n",
        "W8=Wait...\n",
        "7K=Sick:-D Laugher\n",
        "\"\"\"\n",
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "# Chat words removal\n",
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "nm-xdXBIvn3M"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Punctuation removal:\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "#Stop words removal:\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "def lemmatize_words(text):\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "\n",
        "# Emojis Removal\n",
        "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "# Emoticons removal\n",
        "def convert_emoticons(text):\n",
        "    for emot in EMOTICONS:\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
        "    return text\n",
        "\n",
        "# URL and Html tag removal\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    t= url_pattern.sub(r'', text)\n",
        "    html=re.compile(r'<.*?>') \n",
        "    return html.sub(r'',t) #Removing html tags\n",
        "\n",
        "def chat_words_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "#cleaning steps:\n",
        "    # 0- lowercase\n",
        "    # 1- lowercase, punctuation, stopwords\n",
        "    # 2- lowercase, punctuation, stopwords,lemma,,url/tags\n",
        "    # 3- lowercase, punctuation, stopwords,lemma,,url/tags,emoji,chatwords\n",
        "    \n",
        "\n",
        "# step 0\n",
        "def clean_text0(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# step 1\n",
        "def clean_text1(text):\n",
        "    \n",
        "    text = text.lower()\n",
        "    text= remove_punctuation(text)\n",
        "    text=remove_stopwords(text)\n",
        "  \n",
        "    return text\n",
        "\n",
        "# step 2\n",
        "def clean_text2(text):\n",
        "    # Chat words removal\n",
        "    chat_words_map_dict = {}\n",
        "    chat_words_list = []\n",
        "    for line in chat_words_str.split(\"\\n\"):\n",
        "        if line != \"\":\n",
        "            cw = line.split(\"=\")[0]\n",
        "            cw_expanded = line.split(\"=\")[1]\n",
        "            chat_words_list.append(cw)\n",
        "            chat_words_map_dict[cw] = cw_expanded\n",
        "    chat_words_list = set(chat_words_list)\n",
        "    text = text.lower()\n",
        "    text= remove_punctuation(text)\n",
        "    text=remove_stopwords(text)\n",
        "    #text=stem_words(text)\n",
        "    text=lemmatize_words(text)\n",
        "    text= remove_urls(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "# step 3\n",
        "def clean_text3(text):\n",
        "    # Chat words removal\n",
        "    chat_words_map_dict = {}\n",
        "    chat_words_list = []\n",
        "    for line in chat_words_str.split(\"\\n\"):\n",
        "        if line != \"\":\n",
        "            cw = line.split(\"=\")[0]\n",
        "            cw_expanded = line.split(\"=\")[1]\n",
        "            chat_words_list.append(cw)\n",
        "            chat_words_map_dict[cw] = cw_expanded\n",
        "    chat_words_list = set(chat_words_list)\n",
        "    text = text.lower()\n",
        "    text= remove_punctuation(text)\n",
        "    text=remove_stopwords(text)\n",
        "    #text=stem_words(text)\n",
        "    text=lemmatize_words(text)\n",
        "    text=remove_urls(text)\n",
        "    text=remove_emoji(text)\n",
        "    text=convert_emoticons(text)\n",
        "    text=chat_words_conversion(text)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cAb-Pk4YvnpT",
        "outputId": "b4c4c5ba-8f12-48e5-f956-b78871990b49"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>damn write pretti chaotic</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>yeah appar bunch misogynist virgin one turn ga...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fuck woman still mp</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>understand know right time know isnt enough mo...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>surpriz didnt stop rape woman</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text       label\n",
              "0                          damn write pretti chaotic  not sexist\n",
              "1  yeah appar bunch misogynist virgin one turn ga...  not sexist\n",
              "2                                fuck woman still mp  not sexist\n",
              "3  understand know right time know isnt enough mo...  not sexist\n",
              "4                      surpriz didnt stop rape woman  not sexist"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Finalizing preprocessing step:\n",
        "cleaning_step=0\n",
        "if cleaning_step==0:\n",
        "    # cleaning step 0\n",
        "    df[\"cleaned_text\"] = df[\"text\"].apply(lambda x: clean_text0(x))\n",
        "elif cleaning_step==1:\n",
        "    # cleaning step 1\n",
        "    df[\"cleaned_text\"] = df[\"text\"].apply(lambda x: clean_text1(x))\n",
        "elif cleaning_step==2:\n",
        "    # cleaning step 2\n",
        "    df[\"cleaned_text\"] = df[\"text\"].apply(lambda x: clean_text2(x))\n",
        "elif cleaning_step==3:\n",
        "    # cleaning step 3\n",
        "    df[\"cleaned_text\"] = df[\"text\"].apply(lambda x: clean_text3(x))\n",
        "\n",
        "\n",
        "df = df.filter([\"cleaned_text\", \"label\"])\n",
        "df.columns = ['text', 'label']\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>damn write pretti chaotic</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>yeah appar bunch misogynist virgin one turn ga...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fuck woman still mp</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>understand know your right time know isnt enou...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>surpriz didnt stop rape woman</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>yeah order keep benefit good tommorow tell cou...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>hahaha wish would live truth cut shit wouldnt ...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>head mod say talk make post won’t tell anyon t...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>insid wallet cash bank card credit card debit ...</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>definit didn’t go much detail happen put stop ...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>oh shit clearli know big 5 trait deflect use m...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>get initi chanc attract woman flock see woman ...</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>first word presid would ban sport ball forev o...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>sometim im girl reason head bar far im immedi ...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>trump immigr polici eek canadian may revisit t...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>there also enough evid like goldberg antigg si...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>dumb fuck blatantli inflammatori wasnt fuck ad...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>ugli chick pussi pussi</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>fuck nine mm get arab fuck monkey</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>talk shit idiot like nazi like bitch</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>lefti dad offer daughter 100 knock maga hat gu...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>want attack cultur norm go woman call shit</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>alway get suspici woman comment video game fee...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>plu meet ton woman</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>nah black woman still get repli tinder even wh...</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>dumb as woman today ask divorc collect huge al...</td>\n",
              "      <td>sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>neet spend time gymmax studymax it’ good dopam...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>must protect iceland cost especi woman id hate...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>plea elabor even mess woman need get feel smac...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>indian savag wife tie tree 7 hour give 100 las...</td>\n",
              "      <td>not sexist</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text       label\n",
              "0                           damn write pretti chaotic  not sexist\n",
              "1   yeah appar bunch misogynist virgin one turn ga...  not sexist\n",
              "2                                 fuck woman still mp  not sexist\n",
              "3   understand know your right time know isnt enou...  not sexist\n",
              "4                       surpriz didnt stop rape woman  not sexist\n",
              "5   yeah order keep benefit good tommorow tell cou...  not sexist\n",
              "6   hahaha wish would live truth cut shit wouldnt ...  not sexist\n",
              "7   head mod say talk make post won’t tell anyon t...  not sexist\n",
              "8   insid wallet cash bank card credit card debit ...      sexist\n",
              "9   definit didn’t go much detail happen put stop ...  not sexist\n",
              "10  oh shit clearli know big 5 trait deflect use m...  not sexist\n",
              "11  get initi chanc attract woman flock see woman ...      sexist\n",
              "12  first word presid would ban sport ball forev o...  not sexist\n",
              "13  sometim im girl reason head bar far im immedi ...  not sexist\n",
              "14  trump immigr polici eek canadian may revisit t...  not sexist\n",
              "15  there also enough evid like goldberg antigg si...  not sexist\n",
              "16  dumb fuck blatantli inflammatori wasnt fuck ad...  not sexist\n",
              "17                             ugli chick pussi pussi      sexist\n",
              "18                  fuck nine mm get arab fuck monkey      sexist\n",
              "19               talk shit idiot like nazi like bitch      sexist\n",
              "20  lefti dad offer daughter 100 knock maga hat gu...  not sexist\n",
              "21         want attack cultur norm go woman call shit      sexist\n",
              "22  alway get suspici woman comment video game fee...  not sexist\n",
              "23                                 plu meet ton woman  not sexist\n",
              "24  nah black woman still get repli tinder even wh...      sexist\n",
              "25  dumb as woman today ask divorc collect huge al...      sexist\n",
              "26  neet spend time gymmax studymax it’ good dopam...  not sexist\n",
              "27  must protect iceland cost especi woman id hate...  not sexist\n",
              "28  plea elabor even mess woman need get feel smac...  not sexist\n",
              "29  indian savag wife tie tree 7 hour give 100 las...  not sexist"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iDl5MtzUyJ1"
      },
      "source": [
        "### Training SVM/RandomForest/LogisticRegression and Naive Bayesian model using stratified Cross Validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DAT2kVuPgKMR"
      },
      "outputs": [],
      "source": [
        "Encoder = LabelEncoder()\n",
        "X = df['text']\n",
        "y = Encoder.fit_transform(df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def erroranalysis(yp):\n",
        "  for item in yp:\n",
        "    if (item[1]!=item[2]).any():\n",
        "      print(item[0], \"actual:{}  pred:{}\".format(item[1],item[2]),'\\n')\n",
        "\n",
        "d=[]\n",
        "d.append(('sss',1,1))\n",
        "d.append(('ddf',0,1))\n",
        "#erroranalysis(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def f_importances(coef, names):\n",
        "    imp = coef\n",
        "    imp,names = zip(*sorted(zip(imp,names)))\n",
        "    plt.barh(range(len(names)), imp, align='center')\n",
        "    plt.yticks(range(len(names)), names)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zG5_FqmUxFx",
        "outputId": "75b02e91-a8f0-4501-9db4-c95fd2273ff9"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16528\\997390297.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mfeatures_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTfidf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mf_importances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0maccu_skf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mf1_skf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16528\\1605638209.py\u001b[0m in \u001b[0;36mf_importances\u001b[1;34m(coef, names)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mimp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mimp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mbarh\u001b[1;34m(y, width, height, left, align, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbarh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2453\u001b[0m     return gca().barh(\n\u001b[1;32m-> 2454\u001b[1;33m         y, width, height=height, left=left, align=align, **kwargs)\n\u001b[0m\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mbarh\u001b[1;34m(self, y, width, height, left, align, **kwargs)\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'orientation'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'horizontal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2596\u001b[0m         patches = self.bar(x=left, height=height, width=width, bottom=y,\n\u001b[1;32m-> 2597\u001b[1;33m                            align=align, **kwargs)\n\u001b[0m\u001b[0;32m   2598\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1599\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1601\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mbar\u001b[1;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[0;32m   2436\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0morientation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'horizontal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2437\u001b[0m                 \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msticky_edges\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2438\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2439\u001b[0m             \u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_patch\u001b[1;34m(self, p)\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1971\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_patch_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remove_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_patch_limits\u001b[1;34m(self, patch)\u001b[0m\n\u001b[0;32m   1985\u001b[0m         \u001b[1;31m# or height.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1986\u001b[0m         if (isinstance(patch, mpatches.Rectangle) and\n\u001b[1;32m-> 1987\u001b[1;33m                 ((not patch.get_width()) and (not patch.get_height()))):\n\u001b[0m\u001b[0;32m   1988\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1989\u001b[0m         \u001b[0mvertices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m             raise ValueError(\"The truth value of an array with more than one \"\n\u001b[0m\u001b[0;32m    288\u001b[0m                              \"element is ambiguous. Use a.any() or a.all().\")\n\u001b[0;32m    289\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdMUlEQVR4nO3df2zXd53A8Vcp9Ft1a8V167quMMYfowvOSMlxMOqcmZ1omOTuMnaeZTEaZYeRH2cUBI7JhOqmc7m4soPhj0RvIwduWbyeUu/G5AZK5FpjAm63ISvn0SAoLbfFFsrn/jD0rG0n366lvNvHI/n88X3v/fl+39+91/WZz/dHC7IsywIAIAETRnsBAAAXS7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAycg7XH70ox/FwoUL47rrrouCgoJ4+umn/+Q5zz33XNTU1ERxcXHceOON8dhjjw1psQDA+JZ3uLz66qvxjne8I772ta9d1Pxf/vKX8f73vz9qa2ujpaUlPve5z8WnPvWp2LVrV96LBQDGt4I38kcWCwoK4qmnnopFixYNOuezn/1sPPPMM3H48OHesaVLl8bPfvaz2L9//1AfGgAYhyaO9APs378/6urq+ozdeeedsX379jh79mxMmjSp3zldXV3R1dXVe/v8+fPxm9/8Jq666qooKCgY6SUDAMMgy7I4c+ZMXHfddTFhwvC8rXbEw6W9vT3Ky8v7jJWXl8e5c+fi5MmTUVFR0e+choaG+PznPz/SSwMALoFjx47F9ddfPyz3NeLhEhH9rpJceHVqsKsna9asiVWrVvXe7ujoiClTpsSxY8eipKRk5BYKAAybzs7OqKqqiiuvvHLY7nPEw+Xaa6+N9vb2PmMnTpyIiRMnxlVXXTXgOblcLnK5XL/xkpIS4QIAiRnOt3mM+Pe4zJ07N5qbm/uM7d69O2bPnj3g+1sAAAaTd7j87//+b7S2tkZra2tE/P7jzq2trdHW1hYRv3+ZZ8mSJb3zly5dGq+88kqsWrUqDh8+HF//+tdj+/bt8elPf3qYngIAMF7k/VLRT3/607j99tt7b194L8q9994b3/zmN+P48eO9ERMRMW3atGhqaoqVK1fGo48+Gtddd138wz/8Q/zlX/7lMCwfABhP3tD3uFwqnZ2dUVpaGh0dHd7jAgCJGInf3/5WEQCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRhSuDQ2Nsa0adOiuLg4ampqYu/eva87/5FHHombbrop3vSmN0VVVVWsXLkyfve73w1pwQDA+JV3uOzYsSNWrFgRa9eujZaWlqitrY0FCxZEW1vbgPO/853vxOrVq2PDhg1x+PDh2L59e+zYsSPWrFnzhhcPAIwvBVmWZfmcMGfOnJg1a1Zs2bKld6y6ujoWLVoUDQ0N/eZ/8pOfjMOHD8e//du/9Y793d/9XRw4cOBPXqm5oLOzM0pLS6OjoyNKSkryWS4AMEpG4vd3Xldcuru74+DBg1FXV9dnvK6uLvbt2zfgOfPnz4+DBw/GgQMHIiLiyJEj0dTUFB/4wAcGfZyurq7o7OzscwAATMxn8smTJ6OnpyfKy8v7jJeXl0d7e/uA59xzzz3x61//OubPnx9ZlsW5c+fivvvui9WrVw/6OA0NDfH5z38+n6UBAOPAkN6cW1BQ0Od2lmX9xi7Ys2dPbNq0KRobG+M///M/47vf/W5873vfiwceeGDQ+1+zZk10dHT0HseOHRvKMgGAMSavKy5lZWVRWFjY7+rKiRMn+l2FuWD9+vVRX18fH/vYxyIi4u1vf3u8+uqr8fGPfzzWrl0bEyb0b6dcLhe5XC6fpQEA40BeV1yKioqipqYmmpub+4w3NzfHvHnzBjzntdde6xcnhYWFkWVZ5Pm+YABgnMvriktExKpVq6K+vj5mz54dc+fOja1bt0ZbW1ssXbo0IiKWLFkSlZWVvZ8wWrhwYTz88MPxzne+M+bMmRMvvfRSrF+/Pu66664oLCwc3mcDAIxpeYfL4sWL49SpU7Fx48Y4fvx4zJw5M5qammLq1KkREdHW1tbnCsu6deuioKAg1q1bF7/61a/i6quvjoULF8amTZuG71kAAONC3t/jMhp8jwsApGfUv8cFAGA0CRcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIxpDCpbGxMaZNmxbFxcVRU1MTe/fufd35p0+fjmXLlkVFRUUUFxdHdXV1NDU1DWnBAMD4NTHfE3bs2BErVqyIxsbGuPXWW+Mf//EfY8GCBXHo0KGYMmVKv/nd3d3x3ve+N6655prYuXNnXH/99XHs2LG48sorh+UJAADjR0GWZVk+J8yZMydmzZoVW7Zs6R2rrq6ORYsWRUNDQ7/5jz32WDz00EPxi1/8IiZNmjSkRXZ2dkZpaWl0dHRESUnJkO4DALi0RuL3d14vFXV3d8fBgwejrq6uz3hdXV3s27dvwHOeeeaZmDt3bixbtizKy8tj5syZsXnz5ujp6Rn0cbq6uqKzs7PPAQCQV7icPHkyenp6ory8vM94eXl5tLe3D3jOkSNHYufOndHT0xNNTU2xbt26+MpXvhKbNm0a9HEaGhqitLS096iqqspnmQDAGDWkN+cWFBT0uZ1lWb+xC86fPx/XXHNNbN26NWpqauKee+6JtWvX9nmp6Y+tWbMmOjo6eo9jx44NZZkAwBiT15tzy8rKorCwsN/VlRMnTvS7CnNBRUVFTJo0KQoLC3vHqquro729Pbq7u6OoqKjfOblcLnK5XD5LAwDGgbyuuBQVFUVNTU00Nzf3GW9ubo558+YNeM6tt94aL730Upw/f7537MUXX4yKiooBowUAYDB5v1S0atWqePzxx+PrX/96HD58OFauXBltbW2xdOnSiIhYsmRJrFmzpnf+fffdF6dOnYrly5fHiy++GP/yL/8SmzdvjmXLlg3fswAAxoW8v8dl8eLFcerUqdi4cWMcP348Zs6cGU1NTTF16tSIiGhra4sJE/6/h6qqqmL37t2xcuXKuOWWW6KysjKWL18en/3sZ4fvWQAA40Le3+MyGnyPCwCkZ9S/xwUAYDQJFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEjGkMKlsbExpk2bFsXFxVFTUxN79+69qPOefPLJKCgoiEWLFg3lYQGAcS7vcNmxY0esWLEi1q5dGy0tLVFbWxsLFiyItra21z3vlVdeiU9/+tNRW1s75MUCAONbQZZlWT4nzJkzJ2bNmhVbtmzpHauuro5FixZFQ0PDgOf09PTEbbfdFh/5yEdi7969cfr06Xj66acHfYyurq7o6urqvd3Z2RlVVVXR0dERJSUl+SwXABglnZ2dUVpaOqy/v/O64tLd3R0HDx6Murq6PuN1dXWxb9++Qc/buHFjXH311fHRj370oh6noaEhSktLe4+qqqp8lgkAjFF5hcvJkyejp6cnysvL+4yXl5dHe3v7gOc8//zzsX379ti2bdtFP86aNWuio6Oj9zh27Fg+ywQAxqiJQzmpoKCgz+0sy/qNRUScOXMmPvzhD8e2bduirKzsou8/l8tFLpcbytIAgDEsr3ApKyuLwsLCfldXTpw40e8qTETEyy+/HEePHo2FCxf2jp0/f/73DzxxYrzwwgsxffr0oawbABiH8nqpqKioKGpqaqK5ubnPeHNzc8ybN6/f/BkzZsTPf/7zaG1t7T3uuuuuuP3226O1tdV7VwCAvOT9UtGqVauivr4+Zs+eHXPnzo2tW7dGW1tbLF26NCIilixZEpWVldHQ0BDFxcUxc+bMPue/9a1vjYjoNw4A8KfkHS6LFy+OU6dOxcaNG+P48eMxc+bMaGpqiqlTp0ZERFtbW0yY4At5AYDhl/f3uIyGkfgcOAAwskb9e1wAAEaTcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkDClcGhsbY9q0aVFcXBw1NTWxd+/eQedu27YtamtrY/LkyTF58uS444474sCBA0NeMAAwfuUdLjt27IgVK1bE2rVro6WlJWpra2PBggXR1tY24Pw9e/bEX//1X8ezzz4b+/fvjylTpkRdXV386le/esOLBwDGl4Isy7J8TpgzZ07MmjUrtmzZ0jtWXV0dixYtioaGhj95fk9PT0yePDm+9rWvxZIlSwac09XVFV1dXb23Ozs7o6qqKjo6OqKkpCSf5QIAo6SzszNKS0uH9fd3Xldcuru74+DBg1FXV9dnvK6uLvbt23dR9/Haa6/F2bNn421ve9ugcxoaGqK0tLT3qKqqymeZAMAYlVe4nDx5Mnp6eqK8vLzPeHl5ebS3t1/UfaxevToqKyvjjjvuGHTOmjVroqOjo/c4duxYPssEAMaoiUM5qaCgoM/tLMv6jQ3kwQcfjCeeeCL27NkTxcXFg87L5XKRy+WGsjQAYAzLK1zKysqisLCw39WVEydO9LsK88e+/OUvx+bNm+OHP/xh3HLLLfmvFAAY9/J6qaioqChqamqiubm5z3hzc3PMmzdv0PMeeuiheOCBB+L73/9+zJ49e2grBQDGvbxfKlq1alXU19fH7NmzY+7cubF169Zoa2uLpUuXRkTEkiVLorKysvcTRg8++GCsX78+/umf/iluuOGG3qs1V1xxRVxxxRXD+FQAgLEu73BZvHhxnDp1KjZu3BjHjx+PmTNnRlNTU0ydOjUiItra2mLChP+/kNPY2Bjd3d3xV3/1V33uZ8OGDXH//fe/sdUDAONK3t/jMhpG4nPgAMDIGvXvcQEAGE3CBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIxpHBpbGyMadOmRXFxcdTU1MTevXtfd/6uXbvi5ptvjlwuFzfffHM89dRTQ1osADC+5R0uO3bsiBUrVsTatWujpaUlamtrY8GCBdHW1jbg/P3798fixYujvr4+fvazn0V9fX3cfffd8ZOf/OQNLx4AGF8KsizL8jlhzpw5MWvWrNiyZUvvWHV1dSxatCgaGhr6zV+8eHF0dnbGv/7rv/aOve9974vJkyfHE088MeBjdHV1RVdXV+/tjo6OmDJlShw7dixKSkryWS4AMEo6OzujqqoqTp8+HaWlpcNzp1keurq6ssLCwuy73/1un/FPfepT2bve9a4Bz6mqqsoefvjhPmMPP/xwNmXKlEEfZ8OGDVlEOBwOh8PhGAPHyy+/nE9uvK6JkYeTJ09GT09PlJeX9xkvLy+P9vb2Ac9pb2/Pa35ExJo1a2LVqlW9t0+fPh1Tp06Ntra24Ss2huRCPbv6NfrsxeXDXlxe7Mfl48IrJm9729uG7T7zCpcLCgoK+tzOsqzf2BuZn8vlIpfL9RsvLS31H+FloqSkxF5cJuzF5cNeXF7sx+VjwoTh+xBzXvdUVlYWhYWF/a6WnDhxot9VlQuuvfbavOYDAAwmr3ApKiqKmpqaaG5u7jPe3Nwc8+bNG/CcuXPn9pu/e/fuQecDAAym8P77778/nxNKSkpi/fr1UVlZGcXFxbF58+Z49tln4xvf+Ea89a1vjSVLlsSBAwfijjvuiIiIysrKWLduXeRyuSgrK4vt27fH448/Hlu3bo3rr7/+4hdaWBjvfve7Y+LEIb26xTCyF5cPe3H5sBeXF/tx+Rjuvcj749ARv/8CugcffDCOHz8eM2fOjK9+9avxrne9KyIi3v3ud8cNN9wQ3/zmN3vn79y5M9atWxdHjhyJ6dOnx6ZNm+Iv/uIvhuUJAADjx5DCBQBgNPhbRQBAMoQLAJAM4QIAJEO4AADJuGzCpbGxMaZNmxbFxcVRU1MTe/fufd35u3btiptvvjlyuVzcfPPN8dRTT12ilY59+ezFtm3bora2NiZPnhyTJ0+OO+64Iw4cOHAJVzu25ftzccGTTz4ZBQUFsWjRohFe4fiR716cPn06li1bFhUVFVFcXBzV1dXR1NR0iVY7tuW7F4888kjcdNNN8aY3vSmqqqpi5cqV8bvf/e4SrXbs+tGPfhQLFy6M6667LgoKCuLpp5/+k+c899xzUVNTE8XFxXHjjTfGY489lv8DD9tfPXoDnnzyyWzSpEnZtm3bskOHDmXLly/P3vKWt2SvvPLKgPP37duXFRYWZps3b84OHz6cbd68OZs4cWL24x//+BKvfOzJdy8+9KEPZY8++mjW0tKSHT58OPvIRz6SlZaWZv/93/99iVc+9uS7FxccPXo0q6yszGpra7MPfvCDl2i1Y1u+e9HV1ZXNnj07e//735/9x3/8R3b06NFs7969WWtr6yVe+diT7158+9vfznK5XPad73wn++Uvf5n94Ac/yCoqKrIVK1Zc4pWPPU1NTdnatWuzXbt2ZRGRPfXUU687/8iRI9mb3/zmbPny5dmhQ4eybdu2ZZMmTcp27tyZ1+NeFuHyZ3/2Z9nSpUv7jM2YMSNbvXr1gPPvvvvu7H3ve1+fsTvvvDO75557RmyN40W+e/HHzp07l1155ZXZt771rZFY3rgylL04d+5cduutt2aPP/54du+99wqXYZLvXmzZsiW78cYbs+7u7kuxvHEl371YtmxZ9p73vKfP2KpVq7L58+eP2BrHo4sJl8985jPZjBkz+ox94hOfyP78z/88r8ca9ZeKuru74+DBg1FXV9dnvK6uLvbt2zfgOfv37+83/8477xx0PhdnKHvxx1577bU4e/bssP4l0PFoqHuxcePGuPrqq+OjH/3oSC9x3BjKXjzzzDMxd+7cWLZsWZSXl8fMmTNj8+bN0dPTcymWPGYNZS/mz58fBw8e7H0J+8iRI9HU1BQf+MAHRny99DXY7+6f/vSncfbs2Yu+n1H/LuSTJ09GT09Pvz+6WF5e3u+PM17Q3t6e13wuzlD24o+tXr06Kisre//kA0MzlL14/vnnY/v27dHa2nopljhuDGUvjhw5Ev/+7/8ef/M3fxNNTU3xX//1X7Fs2bI4d+5c/P3f//2lWPaYNJS9uOeee+LXv/51zJ8/P7Isi3PnzsV9990Xq1evvhRL5g8M9rv73LlzcfLkyaioqLio+xn1cLmgoKCgz+0sy/qNvZH5XLyh/rt98MEH44knnog9e/ZEcXHxSC1vXLnYvThz5kx8+MMfjm3btkVZWdmlWt64ks/Pxfnz5+Oaa66JrVu3RmFhYdTU1MT//M//xEMPPSRchkE+e7Fnz57YtGlTNDY2xpw5c+Kll16K5cuXR0VFRaxfv/5SLJc/MNDeDTT+ekY9XMrKyqKwsLBfLZ84caJfmV1w7bXX5jWfizOUvbjgy1/+cmzevDl++MMfxi233DKSyxwX8t2Ll19+OY4ePRoLFy7sHTt//nxEREycODFeeOGFmD59+sgueoways9FRUVFTJo0KQoLC3vHqquro729Pbq7u6OoqGhE1zxWDWUv1q9fH/X19fGxj30sIiLe/va3x6uvvhof//jHY+3atTFhwqi/Y2LcGOx398SJE+Oqq6666PsZ9R0rKiqKmpqaaG5u7jPe3Nwc8+bNG/CcuXPn9pu/e/fuQedzcYayFxERDz30UDzwwAPx/e9/P2bPnj3SyxwX8t2LGTNmxM9//vNobW3tPe666664/fbbo7W1Naqqqi7V0secofxc3HrrrfHSSy/1xmNExIsvvhgVFRWi5Q0Yyl689tpr/eKksLAwst9/OGXE1kp/g/3unj17dkyaNOni7yivt/KOkAsfb9u+fXt26NChbMWKFdlb3vKW7OjRo1mWZVl9fX2fd4w///zzWWFhYfbFL34xO3z4cPbFL37Rx6GHSb578aUvfSkrKirKdu7cmR0/frz3OHPmzGg9hTEj3734Yz5VNHzy3Yu2trbsiiuuyD75yU9mL7zwQva9730vu+aaa7IvfOELo/UUxox892LDhg3ZlVdemT3xxBPZkSNHst27d2fTp0/P7r777tF6CmPGmTNnspaWlqylpSWLiOzhhx/OWlpaej+avnr16qy+vr53/oWPQ69cuTI7dOhQtn379nQ/Dp1lWfboo49mU6dOzYqKirJZs2Zlzz33XO8/u+2227J77723z/x//ud/zm666aZs0qRJ2YwZM7Jdu3Zd4hWPXfnsxdSpU7OI6Hds2LDh0i98DMr35+IPCZfhle9e7Nu3L5szZ06Wy+WyG2+8Mdu0aVN27ty5S7zqsSmfvTh79mx2//33Z9OnT8+Ki4uzqqqq7G//9m+z3/72t6Ow8rHl2WefHfD//xf+/d97773Zbbfd1uecPXv2ZO985zuzoqKi7IYbbsi2bNmS9+MWZJlrZQBAGkb9PS4AABdLuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDL+D9lddsb+FAtgAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "k=2\n",
        "\n",
        "skf = model_selection.StratifiedKFold(n_splits=k, shuffle=True, random_state=11)  # 20% for dev set in each fold\n",
        "\n",
        "# using TF-IDF vectorizer, 1-gram and 2-gram settings\n",
        "Tfidf_vect = TfidfVectorizer(max_features=10000,ngram_range=(1,2)) \n",
        "Tfidf_vect.fit(X)\n",
        "\n",
        "\n",
        "lrClassifier = LogisticRegression( multi_class='ovr')#C=5e1, solver='saga', multi_class='ovr', random_state=17, n_jobs=4) #optimizer: sag, saga, lbfgs   multi-class=ovr -> for binary classification\n",
        "svmClassifier=svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "#svmClassifier_s=svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "rfClassifier=RandomForestClassifier()#max_depth=50, random_state=0)\n",
        "\n",
        "models=[lrClassifier,svmClassifier,rfClassifier]\n",
        "#models=[svmClassifier]\n",
        "for model in models:\n",
        "  accu_skf = []\n",
        "  f1_skf = []\n",
        "  precision_skf = []\n",
        "  recall_skf = []\n",
        "  p=[]\n",
        "\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "    X_train_fold, X_test_fold = Tfidf_vect.transform(X[train_index]), Tfidf_vect.transform(X[test_index]) \n",
        "    y_train_fold, y_test_fold = y[train_index], y[test_index] \n",
        "    model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    pred = model.predict(X_test_fold)\n",
        "     \n",
        "    #features_names=Tfidf_vect.inverse_transform(X[test_index])\n",
        "    #f_importances(model.coef_, features_names)  \n",
        "    accu_skf.append(metrics.accuracy_score(pred, y_test_fold))\n",
        "    f1_skf.append(metrics.f1_score(pred, y_test_fold, average = 'macro'))\n",
        "    precision_skf.append(metrics.precision_score(pred, y_test_fold, average = 'macro'))\n",
        "    recall_skf.append(metrics.recall_score(pred, y_test_fold, average = 'macro'))\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  print('model name: {} \\n accuracy:{}  f1: {}   \\n precision:   {}   recall :{}'.format(type(model).__name__,statistics.mean(accu_skf),statistics.mean(f1_skf),statistics.mean(precision_skf),statistics.mean(recall_skf)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIikKTxcIJWj"
      },
      "source": [
        "## Training SVM model without stratified Cross Validation:\n",
        "## Prepare Train and Test Data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSHjjKMPFjS7"
      },
      "outputs": [],
      "source": [
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(df['text'],df['label'],test_size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCLoE_ejIOLh"
      },
      "source": [
        "# Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qylFBGjH3iC"
      },
      "outputs": [],
      "source": [
        "Encoder = LabelEncoder()\n",
        "y_train = Encoder.fit_transform(Train_Y)\n",
        "y_test = Encoder.fit_transform(Test_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QonSwJ4AIYlB"
      },
      "source": [
        "# Word Vectorization (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm_zA3cqIXbB"
      },
      "outputs": [],
      "source": [
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(df['text'])\n",
        "X_train = Tfidf_vect.transform(Train_X)\n",
        "X_test = Tfidf_vect.transform(Test_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9a_PN3JPXh"
      },
      "source": [
        "## Classifier - SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ILebB-rLar8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# fit the training dataset on the classifier\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(X_train,y_train)\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-CGDZXnMehl",
        "outputId": "7f6a6cbe-66a9-4790-ccb1-eaba6e086367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8378571428571429\n",
            "F1: 0.7211712989126149\n",
            "Precision: 0.6864923831405788\n",
            "Recall: 0.857807211686592\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"Accuracy:\", metrics.accuracy_score(predictions_SVM, y_test))\n",
        "print(\"F1:\",metrics.f1_score(predictions_SVM, y_test, average=\"macro\"))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(predictions_SVM, y_test, average=\"macro\"))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(predictions_SVM, y_test, average=\"macro\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVks4NnkMmx-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
