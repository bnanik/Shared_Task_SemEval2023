{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnanik/Shared_Task_SemEval2023/blob/main/prompt_based_classifier_Data_manipulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS-DcKcYGjHe"
      },
      "outputs": [],
      "source": [
        "  !pip install transformers datasets openprompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VASmD2osgGMT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "from transformers import AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_data_file_path = '/content/NewAugmented_dataset.csv'\n",
        "combined_data_file_path = '/content/NewCombined_train_file.csv'\n",
        "validation_data_file_path = '/content/NewTest_set_EDOS.csv'\n",
        "preprocessed_data_file_path = '/content/New_preprocessed_combined_train_file.csv'\n",
        "oversampled_data_file_path = '/content/NewOversampled_train.csv'\n",
        "dev_data_file_path = '/content/dev_task_a_entries.csv'\n",
        "test_data_file_path = '/content/test_task_a_entries.csv'"
      ],
      "metadata": {
        "id": "M92gzkN95d52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TunCAy6H7IuP"
      },
      "outputs": [],
      "source": [
        "# df_train = pd.read_csv(combined_data_file_path) # For using combined data as training data\n",
        "# df_train = pd.read_csv(augmented_data_file_path) # For using augmented data as training data\n",
        "df_train = pd.read_csv(oversampled_data_file_path) # For using oversampled data as training data\n",
        "# df_train = pd.read_csv(downsampled_data_file_path) # For using downsampled data as training data\n",
        "\n",
        "df_train.rename(columns={\"sexist\": \"labels\"}, inplace=True) # uncomment if combined or augmented data is used \n",
        "\n",
        "df_validation = pd.read_csv(validation_data_file_path)\n",
        "# df_validation[\"text\"] = df_validation[\"text\"].astype(str).str.lower()\n",
        "df_validation.rename(columns={\"label_sexist\": \"labels\"}, inplace=True)\n",
        "\n",
        "df_dev = pd.read_csv(dev_data_file_path)\n",
        "# df_dev['text'] = df_dev['text'].astype(str).str.lower()\n",
        "df_dev['labels'] = 0 #we do not use these labels. we just need the labels column\n",
        "\n",
        "df_test_codalab = pd.read_csv(test_data_file_path)\n",
        "# df_test_codalab[\"text\"] = df_test_codalab[\"text\"].astype(str).str.lower()\n",
        "df_test_codalab['labels'] = 0 #we do not use these labels. we just need the labels column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iISPHPRbGYZb"
      },
      "source": [
        "### Encoding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jADyWFVzWMLp"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df_train['labels'] = le.fit_transform(df_train['labels'])\n",
        "df_validation['labels'] = le.fit_transform(df_validation['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWikgZ6Xils3",
        "outputId": "5f149ecb-5f87-42d1-9e49-0b76c010e126"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'labels'],\n",
              "        num_rows: 36153\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'labels'],\n",
              "        num_rows: 2800\n",
              "    })\n",
              "    dev: Dataset({\n",
              "        features: ['rewire_id', 'text', 'labels'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['rewire_id', 'text', 'labels'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "raw_dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train),\n",
        "    \"validation\": Dataset.from_pandas(df_validation),\n",
        "    \"dev\": Dataset.from_pandas(df_dev),\n",
        "    \"test\": Dataset.from_pandas(df_test_codalab)\n",
        "})\n",
        "    \n",
        "\n",
        "raw_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1iGL5weizy4",
        "outputId": "7e294e45-6810-4ad7-a64c-4a9158a71cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"guid\": null,\n",
            "  \"label\": 1,\n",
            "  \"meta\": {},\n",
            "  \"text_a\": \"thing son shell impaling pole hell\",\n",
            "  \"text_b\": \"\",\n",
            "  \"tgt_text\": null\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from openprompt.data_utils import InputExample\n",
        "\n",
        "dataset = {}\n",
        "for split in ['train', 'validation', 'dev', 'test']:\n",
        "    dataset[split] = []\n",
        "    for data in raw_dataset[split]:\n",
        "        input_example = InputExample(text_a = data['text'], label=int(data['labels'])) #, guid=data['__index_level_0__']\n",
        "        dataset[split].append(input_example)\n",
        "print(dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88NLIerMl03m"
      },
      "source": [
        "### Load the PLM related things provided by openprompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CrktWqHn0Hy"
      },
      "outputs": [],
      "source": [
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox2SNck6mC55"
      },
      "source": [
        "### Constructing Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxmPJlA5mCr7"
      },
      "outputs": [],
      "source": [
        "from openprompt.prompts import ManualTemplate\n",
        "template_text = '{\"placeholder\":\"text_a\"}. This text is {\"mask\"}.'\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydb3tn4ctPe-"
      },
      "source": [
        "To better understand how does the template wrap the example, we visualize one instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj2dKXiamCpU",
        "outputId": "50ab3d80-2790-4363-86da-b0d7e8a23dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'text': 'thing son shell impaling pole hell', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '. This text is', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}], {'label': 1}]\n"
          ]
        }
      ],
      "source": [
        "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
        "print(wrapped_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OJFws3lmCnG"
      },
      "outputs": [],
      "source": [
        "# Now, the wrapped example is ready to be pass into the tokenizer, hence producing the input for language models.\n",
        "# You can use the tokenizer to tokenize the input by yourself, but we recommend using our wrapped tokenizer, which is a wrapped tokenizer tailed for InputExample.\n",
        "# The wrapper has been given if you use our `load_plm` function, otherwise, you should choose the suitable wrapper based on\n",
        "# the configuration in `openprompt.plms.__init__.py`.\n",
        "# Note that when t5 is used for classification, we only need to pass <pad> <extra_id_0> <eos> to decoder.\n",
        "# The loss is calcaluted at <extra_id_0>. Thus passing decoder_max_length=3 saves the space\n",
        "\n",
        "wrapped_t5tokenizer = WrapperClass(max_seq_length=512, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwXUDYFSmCkW",
        "outputId": "662cdfa4-29cb-44d9-b0db-7a6fa6fa6050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [589, 520, 7300, 4840, 9, 697, 11148, 7927, 3, 5, 100, 1499, 19, 32099, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'decoder_input_ids': [0, 32099, 0], 'loss_ids': [0, 1, 0]}\n",
            "['▁thing', '▁son', '▁shell', '▁imp', 'a', 'ling', '▁pole', '▁hell', '▁', '.', '▁This', '▁text', '▁is', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<pad>', '<extra_id_0>', '<pad>']\n"
          ]
        }
      ],
      "source": [
        "# You can see what a tokenized example looks like by\n",
        "tokenized_example = wrapped_t5tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)\n",
        "print(tokenized_example)\n",
        "print(tokenizer.convert_ids_to_tokens(tokenized_example['input_ids']))\n",
        "print(tokenizer.convert_ids_to_tokens(tokenized_example['decoder_input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cudk84QOt0I0"
      },
      "source": [
        "Let's convert the whole dataset into the input format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CI_6pPDkmCh4"
      },
      "outputs": [],
      "source": [
        "model_inputs = {}\n",
        "for split in ['train', 'validation', 'dev', 'test']:\n",
        "    model_inputs[split] = []\n",
        "    for sample in dataset[split]:\n",
        "        tokenized_example = wrapped_t5tokenizer.tokenize_one_example(mytemplate.wrap_one_example(sample), teacher_forcing=False)\n",
        "        model_inputs[split].append(tokenized_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DAXXInlEw97"
      },
      "source": [
        "### Define a DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w8uKBBKmCfQ",
        "outputId": "42cdf534-c5ac-40c4-e286-a4fc69588f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 36153it [00:39, 923.49it/s]\n"
          ]
        }
      ],
      "source": [
        "from openprompt import PromptDataLoader\n",
        "\n",
        "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2c4HjVXuv9M",
        "outputId": "e1626c1d-b525-4c2d-d8ae-e1cc35e1d531"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['not sexist', 'sexist'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "le.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ta7CxcCFO7G"
      },
      "source": [
        "## Define the verbalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLh4X2cCmCcd",
        "outputId": "61e7cf54-c5dd-46fc-9c01-eef3eb434a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[   59,     3,     7, 12135]],\n",
            "\n",
            "        [[    3,     7, 12135,     0]]])\n",
            "tensor([[-0.1723, -1.8434],\n",
            "        [-0.4225, -1.0654]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# In classification, we need to define the verbalizer, which is a mapping from logits on the vocabulary to the final label probability. Let's have a look at the verbalizer details:\n",
        "\n",
        "from openprompt.prompts import ManualVerbalizer\n",
        "import torch\n",
        "\n",
        "# for example the verbalizer contains multiple label words in each class\n",
        "myverbalizer = ManualVerbalizer(tokenizer, num_classes=2,\n",
        "                        label_words=[[\"not sexist\"], [\"sexist\"]])\n",
        "\n",
        "print(myverbalizer.label_words_ids)\n",
        "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and\n",
        "print(myverbalizer.process_logits(logits)) # see what the verbalizer do\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLWuqE0JvZ3E"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnQnH6CNmCaI",
        "outputId": "f2b50812-ef01-40ca-9fc5-94040b264ca8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, average loss: 0.7563540935516357\n",
            "Epoch 0, average loss: 0.6196256554594227\n",
            "Epoch 0, average loss: 0.5687805953813662\n",
            "Epoch 0, average loss: 0.5280648420830041\n",
            "Epoch 0, average loss: 0.5062241838661148\n",
            "Epoch 0, average loss: 0.47813740345571504\n",
            "Epoch 0, average loss: 0.46153148549277817\n",
            "Epoch 0, average loss: 0.4519496567247494\n",
            "Epoch 0, average loss: 0.44552478267511636\n",
            "Epoch 0, average loss: 0.44135756894198563\n",
            "Epoch 0, average loss: 0.43677393339124476\n",
            "Epoch 0, average loss: 0.43029626184979797\n",
            "Epoch 0, average loss: 0.4211646255807712\n",
            "Epoch 0, average loss: 0.4169866461449871\n",
            "Epoch 0, average loss: 0.40996529425251244\n",
            "Epoch 0, average loss: 0.4093239647020076\n",
            "Epoch 0, average loss: 0.4068995887755455\n",
            "Epoch 0, average loss: 0.40553273290470154\n",
            "Epoch 0, average loss: 0.3997118576268534\n",
            "Epoch 0, average loss: 0.39505803062664746\n",
            "Epoch 0, average loss: 0.3923602589840101\n",
            "Epoch 0, average loss: 0.38739639584690183\n",
            "Epoch 0, average loss: 0.38343786580177647\n",
            "Epoch 0, average loss: 0.38040192747026597\n",
            "Epoch 0, average loss: 0.3784310927678157\n",
            "Epoch 0, average loss: 0.3751385878350178\n",
            "Epoch 0, average loss: 0.37393680569120924\n",
            "Epoch 0, average loss: 0.37391594315890764\n",
            "Epoch 0, average loss: 0.372278706779867\n",
            "Epoch 0, average loss: 0.37004107434134403\n",
            "Epoch 0, average loss: 0.36602560691563396\n",
            "Epoch 0, average loss: 0.36385337190914524\n",
            "Epoch 0, average loss: 0.3621217635148109\n",
            "Epoch 0, average loss: 0.3593641641707149\n",
            "Epoch 0, average loss: 0.3555035302399065\n",
            "Epoch 0, average loss: 0.35363419173129257\n",
            "Epoch 0, average loss: 0.3519326120307064\n",
            "Epoch 0, average loss: 0.34835282148481894\n",
            "Epoch 0, average loss: 0.3464222982037628\n",
            "Epoch 0, average loss: 0.3439516875806237\n",
            "Epoch 0, average loss: 0.34212959441565804\n",
            "Epoch 0, average loss: 0.3409081107719067\n",
            "Epoch 0, average loss: 0.3387969988128964\n",
            "Epoch 0, average loss: 0.3361556385811016\n",
            "Epoch 0, average loss: 0.3347431101395885\n",
            "Epoch 0, average loss: 0.332052318750095\n",
            "Epoch 0, average loss: 0.3311728537830695\n",
            "Epoch 0, average loss: 0.3292134212645558\n",
            "Epoch 0, average loss: 0.3282793891247525\n",
            "Epoch 0, average loss: 0.32642313109601856\n",
            "Epoch 0, average loss: 0.32580071155113394\n",
            "Epoch 0, average loss: 0.3237926077518593\n",
            "Epoch 0, average loss: 0.3217998967280949\n",
            "Epoch 0, average loss: 0.320526330997619\n",
            "Epoch 0, average loss: 0.31934627389936315\n",
            "Epoch 0, average loss: 0.31858714021563256\n",
            "Epoch 0, average loss: 0.31677238677682534\n",
            "Epoch 0, average loss: 0.3157163066262352\n",
            "Epoch 0, average loss: 0.3134415057612267\n",
            "Epoch 0, average loss: 0.31140113847865303\n",
            "Epoch 0, average loss: 0.31011839953031\n",
            "Epoch 0, average loss: 0.3089867642209273\n",
            "Epoch 0, average loss: 0.3075722057565256\n",
            "Epoch 0, average loss: 0.3068615642086554\n",
            "Epoch 0, average loss: 0.30520755576539127\n",
            "Epoch 0, average loss: 0.3028437667528185\n",
            "Epoch 0, average loss: 0.30070283835244654\n",
            "Epoch 0, average loss: 0.29904293672190735\n",
            "Epoch 0, average loss: 0.2974354851600223\n",
            "Epoch 0, average loss: 0.2957224944184059\n",
            "Epoch 0, average loss: 0.29438971810268005\n",
            "Epoch 0, average loss: 0.29287688214097884\n",
            "Epoch 0, average loss: 0.2918479260465699\n",
            "Epoch 0, average loss: 0.2907355842745169\n",
            "Epoch 0, average loss: 0.28915973699318426\n",
            "Epoch 0, average loss: 0.28785259436957933\n",
            "Epoch 0, average loss: 0.2857846033083496\n",
            "Epoch 0, average loss: 0.28498266668218725\n",
            "Epoch 0, average loss: 0.28346672283170216\n",
            "Epoch 0, average loss: 0.2819348395679301\n",
            "Epoch 0, average loss: 0.28085795479635\n",
            "Epoch 0, average loss: 0.28021106835685694\n",
            "Epoch 0, average loss: 0.2793350562254021\n",
            "Epoch 0, average loss: 0.2777658309109521\n",
            "Epoch 0, average loss: 0.27705489238409586\n",
            "Epoch 0, average loss: 0.2756386347707742\n",
            "Epoch 0, average loss: 0.27413232303649726\n",
            "Epoch 0, average loss: 0.273353884013616\n",
            "Epoch 0, average loss: 0.27246061002742283\n",
            "Epoch 0, average loss: 0.2713961786136677\n",
            "Epoch 0, average loss: 0.27006806657638505\n",
            "Epoch 1, average loss: 0.04110829904675484\n",
            "Epoch 1, average loss: 0.07786973749941406\n",
            "Epoch 1, average loss: 0.0765937398391543\n",
            "Epoch 1, average loss: 0.07527044668615827\n",
            "Epoch 1, average loss: 0.07821260494206195\n",
            "Epoch 1, average loss: 0.07886979137582245\n",
            "Epoch 1, average loss: 0.08151750300078182\n",
            "Epoch 1, average loss: 0.07996068083010327\n",
            "Epoch 1, average loss: 0.08119034893214556\n",
            "Epoch 1, average loss: 0.08200928169648887\n",
            "Epoch 1, average loss: 0.08386704653552253\n",
            "Epoch 1, average loss: 0.08471007296657344\n",
            "Epoch 1, average loss: 0.08456484529502184\n",
            "Epoch 1, average loss: 0.08411163418142717\n",
            "Epoch 1, average loss: 0.08336177364056463\n",
            "Epoch 1, average loss: 0.08129544689078022\n",
            "Epoch 1, average loss: 0.08437497092598689\n",
            "Epoch 1, average loss: 0.08483621028397285\n",
            "Epoch 1, average loss: 0.08467841227038167\n",
            "Epoch 1, average loss: 0.08348724002820464\n",
            "Epoch 1, average loss: 0.08344227772196844\n",
            "Epoch 1, average loss: 0.08679044506758435\n",
            "Epoch 1, average loss: 0.0856051586347212\n",
            "Epoch 1, average loss: 0.08565554241145641\n",
            "Epoch 1, average loss: 0.08618757013418092\n",
            "Epoch 1, average loss: 0.08549247464536026\n",
            "Epoch 1, average loss: 0.08513501844429432\n",
            "Epoch 1, average loss: 0.08472162186240775\n",
            "Epoch 1, average loss: 0.08513569088097783\n",
            "Epoch 1, average loss: 0.08453113658767959\n",
            "Epoch 1, average loss: 0.08480063153706714\n",
            "Epoch 1, average loss: 0.08513098873365807\n",
            "Epoch 1, average loss: 0.08430575514991247\n",
            "Epoch 1, average loss: 0.08399629780696964\n",
            "Epoch 1, average loss: 0.0849830669392161\n",
            "Epoch 1, average loss: 0.08450157497653185\n",
            "Epoch 1, average loss: 0.08525117623617448\n",
            "Epoch 1, average loss: 0.08446648418782321\n",
            "Epoch 1, average loss: 0.08439993517677717\n",
            "Epoch 1, average loss: 0.08454662538882228\n",
            "Epoch 1, average loss: 0.08446854602161688\n",
            "Epoch 1, average loss: 0.0843107298775543\n",
            "Epoch 1, average loss: 0.08440608448329345\n",
            "Epoch 1, average loss: 0.08453628526708824\n",
            "Epoch 1, average loss: 0.08438187432072186\n",
            "Epoch 1, average loss: 0.08446830153044753\n",
            "Epoch 1, average loss: 0.08486645354189519\n",
            "Epoch 1, average loss: 0.08486057191875382\n",
            "Epoch 1, average loss: 0.08584236679096706\n",
            "Epoch 1, average loss: 0.08524051425069631\n",
            "Epoch 1, average loss: 0.08481001023574718\n",
            "Epoch 1, average loss: 0.08436303666153387\n",
            "Epoch 1, average loss: 0.08403743994388577\n",
            "Epoch 1, average loss: 0.08457548144218326\n",
            "Epoch 1, average loss: 0.08407521744618933\n",
            "Epoch 1, average loss: 0.08382755206514622\n",
            "Epoch 1, average loss: 0.08381769224359259\n",
            "Epoch 1, average loss: 0.08374241932952814\n",
            "Epoch 1, average loss: 0.08394849602654914\n",
            "Epoch 1, average loss: 0.08354762708681336\n",
            "Epoch 1, average loss: 0.08350077761280966\n",
            "Epoch 1, average loss: 0.0829315887234945\n",
            "Epoch 1, average loss: 0.08329969486999736\n",
            "Epoch 1, average loss: 0.08301008748330192\n",
            "Epoch 1, average loss: 0.08272470416470883\n",
            "Epoch 1, average loss: 0.08297939982049432\n",
            "Epoch 1, average loss: 0.08256199036106704\n",
            "Epoch 1, average loss: 0.081933867158493\n",
            "Epoch 1, average loss: 0.08127000719478739\n",
            "Epoch 1, average loss: 0.08179337095487034\n",
            "Epoch 1, average loss: 0.08148691589621415\n",
            "Epoch 1, average loss: 0.08137677034353913\n",
            "Epoch 1, average loss: 0.08170785605822058\n",
            "Epoch 1, average loss: 0.08222054140975457\n",
            "Epoch 1, average loss: 0.08185485091922047\n",
            "Epoch 1, average loss: 0.08139468530537672\n",
            "Epoch 1, average loss: 0.08131417424124882\n",
            "Epoch 1, average loss: 0.08151424907664426\n",
            "Epoch 1, average loss: 0.0811850460225193\n",
            "Epoch 1, average loss: 0.08096704129533559\n",
            "Epoch 1, average loss: 0.08123574833617074\n",
            "Epoch 1, average loss: 0.0811233752471055\n",
            "Epoch 1, average loss: 0.08099893406515124\n",
            "Epoch 1, average loss: 0.08138405481416021\n",
            "Epoch 1, average loss: 0.08119252988171162\n",
            "Epoch 1, average loss: 0.08093209852695493\n",
            "Epoch 1, average loss: 0.08092922924779278\n",
            "Epoch 1, average loss: 0.08108107805541993\n",
            "Epoch 1, average loss: 0.08074412934585999\n",
            "Epoch 1, average loss: 0.08063550626010774\n",
            "Epoch 1, average loss: 0.08046236885854414\n",
            "Epoch 2, average loss: 0.011225501773878932\n",
            "Epoch 2, average loss: 0.052027007776903736\n",
            "Epoch 2, average loss: 0.036587933074554144\n",
            "Epoch 2, average loss: 0.02797488737786994\n",
            "Epoch 2, average loss: 0.025878674605819333\n",
            "Epoch 2, average loss: 0.02865570404457098\n",
            "Epoch 2, average loss: 0.029158128955697728\n",
            "Epoch 2, average loss: 0.03552352158363061\n",
            "Epoch 2, average loss: 0.03513074885743951\n",
            "Epoch 2, average loss: 0.03376067858288179\n",
            "Epoch 2, average loss: 0.035605695644636705\n",
            "Epoch 2, average loss: 0.033806967296753476\n",
            "Epoch 2, average loss: 0.03458039728817185\n",
            "Epoch 2, average loss: 0.03595789326381269\n",
            "Epoch 2, average loss: 0.034619771545196056\n",
            "Epoch 2, average loss: 0.03309451771122273\n",
            "Epoch 2, average loss: 0.03495857175967125\n",
            "Epoch 2, average loss: 0.03500381013783961\n",
            "Epoch 2, average loss: 0.03385411932007411\n",
            "Epoch 2, average loss: 0.03365830211108106\n",
            "Epoch 2, average loss: 0.03300461788795908\n",
            "Epoch 2, average loss: 0.03270003923440796\n",
            "Epoch 2, average loss: 0.033550174457977935\n",
            "Epoch 2, average loss: 0.034237580990737614\n",
            "Epoch 2, average loss: 0.03332897197920762\n",
            "Epoch 2, average loss: 0.03276586694445435\n",
            "Epoch 2, average loss: 0.0328571322741886\n",
            "Epoch 2, average loss: 0.032848781700629485\n",
            "Epoch 2, average loss: 0.03409389681991679\n",
            "Epoch 2, average loss: 0.03365666256429923\n",
            "Epoch 2, average loss: 0.03500599821195582\n",
            "Epoch 2, average loss: 0.03551669471667507\n",
            "Epoch 2, average loss: 0.03515606962835341\n",
            "Epoch 2, average loss: 0.03476132264361067\n",
            "Epoch 2, average loss: 0.03453740254299375\n",
            "Epoch 2, average loss: 0.0341728994521885\n",
            "Epoch 2, average loss: 0.03542086785280569\n",
            "Epoch 2, average loss: 0.035301017892844\n",
            "Epoch 2, average loss: 0.036020169810217016\n",
            "Epoch 2, average loss: 0.035790906764982366\n",
            "Epoch 2, average loss: 0.035294876077615694\n",
            "Epoch 2, average loss: 0.035617918062743656\n",
            "Epoch 2, average loss: 0.035209527273715804\n",
            "Epoch 2, average loss: 0.03485397237371936\n",
            "Epoch 2, average loss: 0.035752360805037915\n",
            "Epoch 2, average loss: 0.035874238902229016\n",
            "Epoch 2, average loss: 0.03635818772982431\n",
            "Epoch 2, average loss: 0.036728342463139864\n",
            "Epoch 2, average loss: 0.037096383035669776\n",
            "Epoch 2, average loss: 0.03764104027476267\n",
            "Epoch 2, average loss: 0.037471226685260484\n",
            "Epoch 2, average loss: 0.037969849243941534\n",
            "Epoch 2, average loss: 0.037635995916047355\n",
            "Epoch 2, average loss: 0.03754963587925689\n",
            "Epoch 2, average loss: 0.03747880893675002\n",
            "Epoch 2, average loss: 0.03774261486696092\n",
            "Epoch 2, average loss: 0.03729993307847709\n",
            "Epoch 2, average loss: 0.03836380895024191\n",
            "Epoch 2, average loss: 0.038769273381655193\n",
            "Epoch 2, average loss: 0.03909311218025327\n",
            "Epoch 2, average loss: 0.03951674778844971\n",
            "Epoch 2, average loss: 0.03949052858961025\n",
            "Epoch 2, average loss: 0.03951843055690137\n",
            "Epoch 2, average loss: 0.03940164935118795\n",
            "Epoch 2, average loss: 0.039315508588043155\n",
            "Epoch 2, average loss: 0.039593272578932276\n",
            "Epoch 2, average loss: 0.03933425449267207\n",
            "Epoch 2, average loss: 0.039502076187556065\n",
            "Epoch 2, average loss: 0.039458439035620345\n",
            "Epoch 2, average loss: 0.039417323322124796\n",
            "Epoch 2, average loss: 0.03934283436186991\n",
            "Epoch 2, average loss: 0.03902452612968221\n",
            "Epoch 2, average loss: 0.03922122611259165\n",
            "Epoch 2, average loss: 0.039363639588509636\n",
            "Epoch 2, average loss: 0.039205779207903114\n",
            "Epoch 2, average loss: 0.03922860674670605\n",
            "Epoch 2, average loss: 0.03908259092013271\n",
            "Epoch 2, average loss: 0.03886792012939428\n",
            "Epoch 2, average loss: 0.03881583349957408\n",
            "Epoch 2, average loss: 0.03893123870443212\n",
            "Epoch 2, average loss: 0.038996814918421725\n",
            "Epoch 2, average loss: 0.03889951442531098\n",
            "Epoch 2, average loss: 0.039284477925343064\n",
            "Epoch 2, average loss: 0.03921881103540062\n",
            "Epoch 2, average loss: 0.039110297663434615\n",
            "Epoch 2, average loss: 0.03916785056490688\n",
            "Epoch 2, average loss: 0.03911493356122136\n",
            "Epoch 2, average loss: 0.03913147008375068\n",
            "Epoch 2, average loss: 0.03904808816291061\n",
            "Epoch 2, average loss: 0.038825057564617425\n",
            "Epoch 2, average loss: 0.03895454567386041\n",
            "Epoch 3, average loss: 0.09024032811430516\n",
            "Epoch 3, average loss: 0.02029017784195507\n",
            "Epoch 3, average loss: 0.0242214704528961\n",
            "Epoch 3, average loss: 0.020208326275379048\n",
            "Epoch 3, average loss: 0.020497951645176645\n",
            "Epoch 3, average loss: 0.021944101008559633\n",
            "Epoch 3, average loss: 0.02076419154715156\n",
            "Epoch 3, average loss: 0.024855226564481952\n",
            "Epoch 3, average loss: 0.023803321340937586\n",
            "Epoch 3, average loss: 0.023763392368799034\n",
            "Epoch 3, average loss: 0.02523893073255644\n",
            "Epoch 3, average loss: 0.024163498299027485\n",
            "Epoch 3, average loss: 0.02326659749438569\n",
            "Epoch 3, average loss: 0.021752535052900875\n",
            "Epoch 3, average loss: 0.021530212310865892\n",
            "Epoch 3, average loss: 0.020492125050269136\n",
            "Epoch 3, average loss: 0.021149929089535863\n",
            "Epoch 3, average loss: 0.02108897443377039\n",
            "Epoch 3, average loss: 0.02085845155047438\n",
            "Epoch 3, average loss: 0.02120630065197746\n",
            "Epoch 3, average loss: 0.020854554201702406\n",
            "Epoch 3, average loss: 0.02191086685583508\n",
            "Epoch 3, average loss: 0.023101848523554788\n",
            "Epoch 3, average loss: 0.023815619809928724\n",
            "Epoch 3, average loss: 0.024234755505698015\n",
            "Epoch 3, average loss: 0.02383911888094544\n",
            "Epoch 3, average loss: 0.023728191028656358\n",
            "Epoch 3, average loss: 0.024310722343170343\n",
            "Epoch 3, average loss: 0.023997900648695804\n",
            "Epoch 3, average loss: 0.023250748316509324\n",
            "Epoch 3, average loss: 0.02375568297754848\n",
            "Epoch 3, average loss: 0.023289587343885393\n",
            "Epoch 3, average loss: 0.02277238374458865\n",
            "Epoch 3, average loss: 0.022622372783521884\n",
            "Epoch 3, average loss: 0.02228900288447399\n",
            "Epoch 3, average loss: 0.02274254064736309\n",
            "Epoch 3, average loss: 0.023147295187494074\n",
            "Epoch 3, average loss: 0.023487592487665875\n",
            "Epoch 3, average loss: 0.023086152999315466\n",
            "Epoch 3, average loss: 0.023794933172899813\n",
            "Epoch 3, average loss: 0.02389121930386149\n",
            "Epoch 3, average loss: 0.023858768566542118\n",
            "Epoch 3, average loss: 0.023681093503827803\n",
            "Epoch 3, average loss: 0.02377216832387953\n",
            "Epoch 3, average loss: 0.02407145284195379\n",
            "Epoch 3, average loss: 0.024483003098202002\n",
            "Epoch 3, average loss: 0.024414108709462323\n",
            "Epoch 3, average loss: 0.024976227322612184\n",
            "Epoch 3, average loss: 0.02489778364281755\n",
            "Epoch 3, average loss: 0.02588365291639979\n",
            "Epoch 3, average loss: 0.025697663610279538\n",
            "Epoch 3, average loss: 0.02570415461329313\n",
            "Epoch 3, average loss: 0.0261572639294419\n",
            "Epoch 3, average loss: 0.026275848113436716\n",
            "Epoch 3, average loss: 0.02607549167429182\n",
            "Epoch 3, average loss: 0.025910890280430417\n",
            "Epoch 3, average loss: 0.025699134905399778\n",
            "Epoch 3, average loss: 0.025695836941407653\n",
            "Epoch 3, average loss: 0.025572235128475163\n",
            "Epoch 3, average loss: 0.02562789883339902\n",
            "Epoch 3, average loss: 0.02573894090254156\n",
            "Epoch 3, average loss: 0.02575934258970636\n",
            "Epoch 3, average loss: 0.025729265438604036\n",
            "Epoch 3, average loss: 0.02607233032980417\n",
            "Epoch 3, average loss: 0.025823284995188277\n",
            "Epoch 3, average loss: 0.026157992973172572\n",
            "Epoch 3, average loss: 0.026657677198397567\n",
            "Epoch 3, average loss: 0.02667087928609513\n",
            "Epoch 3, average loss: 0.026708833730713928\n",
            "Epoch 3, average loss: 0.0272331662465682\n",
            "Epoch 3, average loss: 0.027382177433066583\n",
            "Epoch 3, average loss: 0.027493806644408463\n",
            "Epoch 3, average loss: 0.027742715246574616\n",
            "Epoch 3, average loss: 0.02802836993033919\n",
            "Epoch 3, average loss: 0.028042283412020988\n",
            "Epoch 3, average loss: 0.028140623808609915\n",
            "Epoch 3, average loss: 0.028294899015378787\n",
            "Epoch 3, average loss: 0.028257555361062062\n",
            "Epoch 3, average loss: 0.02835574326413973\n",
            "Epoch 3, average loss: 0.02872483856220052\n",
            "Epoch 3, average loss: 0.028630427557078557\n",
            "Epoch 3, average loss: 0.028408042895721446\n",
            "Epoch 3, average loss: 0.02821077659233362\n",
            "Epoch 3, average loss: 0.027999769430502943\n",
            "Epoch 3, average loss: 0.028356591030262664\n",
            "Epoch 3, average loss: 0.028272513580093\n",
            "Epoch 3, average loss: 0.028698754911256858\n",
            "Epoch 3, average loss: 0.02883894134883696\n",
            "Epoch 3, average loss: 0.028967037615147993\n",
            "Epoch 3, average loss: 0.02891605042648498\n",
            "Epoch 3, average loss: 0.02904546291366003\n"
          ]
        }
      ],
      "source": [
        "from openprompt import PromptForClassification\n",
        "\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
        "if use_cuda:\n",
        "    prompt_model=  prompt_model.cuda()\n",
        "\n",
        "# Now the training is standard\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
        "\n",
        "for epoch in range(4):\n",
        "    tot_loss = 0\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        loss = loss_func(logits, labels)\n",
        "        loss.backward()\n",
        "        tot_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if step %100 ==1:\n",
        "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bjYh1OymCXh",
        "outputId": "8ef26da5-95df-4bf5-b9bd-b44e1109d983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 2800it [00:03, 926.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8053571428571429\n"
          ]
        }
      ],
      "source": [
        "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "for step, inputs in enumerate(validation_dataloader):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qxW0pF849-o",
        "outputId": "ae6ed69f-e542-493a-b8b7-4c656a322d17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2800"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(allpreds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "o5PedA46mCU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecea296-5ccf-4c44-8db3-1359533fd75a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8053571428571429\n",
            "F1: 0.7528072014422064\n",
            "Precision: 0.7437397842432167\n",
            "Recall: 0.7652780187369882\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"Accuracy:\", metrics.accuracy_score(alllabels, allpreds))\n",
        "print(\"F1:\",metrics.f1_score(alllabels, allpreds, average=\"macro\"))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(alllabels, allpreds, average=\"macro\"))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(alllabels, allpreds, average=\"macro\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "d4IIcn3hkaUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97357ec6-7c98-4029-c444-df748f1b193c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 2000it [00:02, 808.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7185\n"
          ]
        }
      ],
      "source": [
        "dev_dataloader = PromptDataLoader(dataset=dataset[\"dev\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "\n",
        "dev_allpreds = []\n",
        "dev_alllabels = []\n",
        "for step, inputs in enumerate(dev_dataloader):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    dev_alllabels.extend(labels.cpu().tolist())\n",
        "    dev_allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "acc = sum([int(i==j) for i,j in zip(dev_allpreds, dev_alllabels)])/len(dev_allpreds)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dev.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iTgLv80t69P",
        "outputId": "3a208142-688c-4c6a-9b33-1ecc68aec32e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "S0cHTZ-hkZ9Z"
      },
      "outputs": [],
      "source": [
        "df_dev['label_pred'] = le.inverse_transform(dev_allpreds)\n",
        "df_dev.drop(columns=['labels', 'text'],axis=1, inplace=True)\n",
        "df_dev.to_csv('/content/dev_task_a_t5_base.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ1beDxX5Xf-",
        "outputId": "59a469ae-7fca-4a09-ebe5-0b78727dfc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 4000it [00:04, 909.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.77325\n"
          ]
        }
      ],
      "source": [
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "\n",
        "test_allpreds = []\n",
        "test_alllabels = []\n",
        "for step, inputs in enumerate(test_dataloader):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    test_alllabels.extend(labels.cpu().tolist())\n",
        "    test_allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "acc = sum([int(i==j) for i,j in zip(test_allpreds, test_alllabels)])/len(test_allpreds)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT4xrcSteYAL"
      },
      "outputs": [],
      "source": [
        "df_test_codalab['label_pred'] = le.inverse_transform(test_allpreds)\n",
        "df_test_codalab.drop(columns=['labels', 'text'],axis=1, inplace=True)\n",
        "df_test_codalab.to_csv('/content/test_task_a_labeled_t5_large.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTeNmMwdexTg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fTR6Sv4f88H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3A6CqSESp1GH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6so_gl7ip09o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKnsmy1Dp0w7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}